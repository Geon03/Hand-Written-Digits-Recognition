{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9193d19f-174e-48e6-8648-db53d735b044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\dell\\anaconda3\\lib\\site-packages (4.7.0.72)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\dell\\anaconda3\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dell\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow-intel==2.18.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (74.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\dell\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dell\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\dell\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\dell\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\dell\\anaconda3\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python tensorflow numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cee28c-5a50-4578-a3e9-0e2c4b7bbc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hand Digit Recognition System\n",
      "============================\n",
      "Model file digit_recognition_model.h5 not found\n",
      "No pre-trained model found. Training new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Training model...\n",
      "Epoch 1/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 38ms/step - accuracy: 0.7864 - loss: 0.6746 - val_accuracy: 0.9828 - val_loss: 0.0528\n",
      "Epoch 2/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 37ms/step - accuracy: 0.9759 - loss: 0.0803 - val_accuracy: 0.9867 - val_loss: 0.0385\n",
      "Epoch 3/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 36ms/step - accuracy: 0.9842 - loss: 0.0517 - val_accuracy: 0.9902 - val_loss: 0.0315\n",
      "Epoch 4/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 35ms/step - accuracy: 0.9880 - loss: 0.0383 - val_accuracy: 0.9886 - val_loss: 0.0347\n",
      "Epoch 5/5\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 39ms/step - accuracy: 0.9896 - loss: 0.0331 - val_accuracy: 0.9898 - val_loss: 0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9898\n",
      "Model saved to digit_recognition_model.h5\n",
      "Starting webcam digit recognition...\n",
      "Instructions:\n",
      "- Draw digits in the green rectangle\n",
      "- Press 'c' to clear the drawing area\n",
      "- Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "class HandDigitRecognizer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"Create a CNN model for digit recognition\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train_model(self, epochs=10):\n",
    "        \"\"\"Train the model using MNIST dataset\"\"\"\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        x_train = x_train.astype('float32') / 255.0\n",
    "        x_test = x_test.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape data to add channel dimension\n",
    "        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        history = self.model.fit(x_train, y_train,\n",
    "                                epochs=epochs,\n",
    "                                batch_size=128,\n",
    "                                validation_data=(x_test, y_test),\n",
    "                                verbose=1)\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_acc = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return history\n",
    "    \n",
    "    def save_model(self, filepath='digit_recognition_model.h5'):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.model and self.is_trained:\n",
    "            self.model.save(filepath)\n",
    "            print(f\"Model saved to {filepath}\")\n",
    "        else:\n",
    "            print(\"No trained model to save\")\n",
    "    \n",
    "    def load_model(self, filepath='digit_recognition_model.h5'):\n",
    "        \"\"\"Load a pre-trained model\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            self.model = keras.models.load_model(filepath)\n",
    "            self.is_trained = True\n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "        else:\n",
    "            print(f\"Model file {filepath} not found\")\n",
    "    \n",
    "    def preprocess_image(self, img):\n",
    "        \"\"\"Preprocess image for prediction\"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resize to 28x28\n",
    "        img = cv2.resize(img, (28, 28))\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        img = img.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape for model input\n",
    "        img = img.reshape(1, 28, 28, 1)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def predict_digit(self, img):\n",
    "        \"\"\"Predict digit from image\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return None, 0\n",
    "        \n",
    "        processed_img = self.preprocess_image(img)\n",
    "        prediction = self.model.predict(processed_img, verbose=0)\n",
    "        digit = np.argmax(prediction)\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        return digit, confidence\n",
    "    \n",
    "    def run_webcam_recognition(self):\n",
    "        \"\"\"Run real-time digit recognition from webcam\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model not trained! Please train the model first.\")\n",
    "            return\n",
    "        \n",
    "        # Initialize webcam\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting webcam digit recognition...\")\n",
    "        print(\"Instructions:\")\n",
    "        print(\"- Draw digits in the green rectangle\")\n",
    "        print(\"- Press 'c' to clear the drawing area\")\n",
    "        print(\"- Press 'q' to quit\")\n",
    "        \n",
    "        # Create a blank canvas for drawing\n",
    "        canvas = np.zeros((400, 400), dtype=np.uint8)\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Flip frame horizontally for mirror effect\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Define drawing area\n",
    "            drawing_area = (50, 50, 300, 300)  # x, y, width, height\n",
    "            \n",
    "            # Draw rectangle for drawing area\n",
    "            cv2.rectangle(frame, (drawing_area[0], drawing_area[1]), \n",
    "                         (drawing_area[0] + drawing_area[2], drawing_area[1] + drawing_area[3]), \n",
    "                         (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract region of interest\n",
    "            roi = frame[drawing_area[1]:drawing_area[1] + drawing_area[3],\n",
    "                       drawing_area[0]:drawing_area[0] + drawing_area[2]]\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Apply threshold to get binary image\n",
    "            _, roi_thresh = cv2.threshold(roi_gray, 127, 255, cv2.THRESH_BINARY)\n",
    "            \n",
    "            # Invert image (white digit on black background)\n",
    "            roi_thresh = cv2.bitwise_not(roi_thresh)\n",
    "            \n",
    "            # Predict digit\n",
    "            digit, confidence = self.predict_digit(roi_thresh)\n",
    "            \n",
    "            # Display prediction\n",
    "            if digit is not None and confidence > 0.5:\n",
    "                cv2.putText(frame, f\"Digit: {digit}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 70), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display instructions\n",
    "            cv2.putText(frame, \"Draw digit in green box\", (10, frame.shape[0] - 60), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, \"Press 'c' to clear, 'q' to quit\", (10, frame.shape[0] - 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # Show processed ROI in corner\n",
    "            roi_display = cv2.resize(roi_thresh, (100, 100))\n",
    "            frame[10:110, frame.shape[1]-110:frame.shape[1]-10] = cv2.cvtColor(roi_display, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            cv2.imshow('Hand Digit Recognition', frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                canvas = np.zeros((400, 400), dtype=np.uint8)\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the digit recognition system\"\"\"\n",
    "    recognizer = HandDigitRecognizer()\n",
    "    \n",
    "    print(\"Hand Digit Recognition System\")\n",
    "    print(\"============================\")\n",
    "    \n",
    "    # Try to load existing model\n",
    "    recognizer.load_model()\n",
    "    \n",
    "    # If no model exists, create and train one\n",
    "    if not recognizer.is_trained:\n",
    "        print(\"No pre-trained model found. Training new model...\")\n",
    "        recognizer.create_model()\n",
    "        recognizer.train_model(epochs=5)  # Reduced epochs for faster training\n",
    "        recognizer.save_model()\n",
    "    \n",
    "    # Start webcam recognition\n",
    "    recognizer.run_webcam_recognition()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60cc57-80cc-4538-867c-7490d85a5ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Hand Digit Recognition System\n",
      "=====================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from improved_digit_model.h5\n",
      "\n",
      "Select mode:\n",
      "1. Drawing mode (recommended for testing)\n",
      "2. Webcam mode\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (1 or 2):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting improved webcam digit recognition...\n",
      "Instructions:\n",
      "- Hold up handwritten digits to the camera\n",
      "- Keep digits steady for better recognition\n",
      "- Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from scipy import ndimage\n",
    "\n",
    "class ImprovedHandDigitRecognizer:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.is_trained = False\n",
    "        self.drawing = False\n",
    "        self.canvas = np.zeros((400, 400), dtype=np.uint8)\n",
    "        self.last_prediction = None\n",
    "        self.prediction_history = []\n",
    "        \n",
    "    def create_model(self):\n",
    "        \"\"\"Create an improved CNN model for digit recognition\"\"\"\n",
    "        model = keras.Sequential([\n",
    "            # First Conv Block\n",
    "            layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Second Conv Block\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "            layers.MaxPooling2D((2, 2)),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Third Conv Block\n",
    "            layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.25),\n",
    "            \n",
    "            # Dense layers\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.BatchNormalization(),\n",
    "            layers.Dropout(0.5),\n",
    "            layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='sparse_categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def augment_data(self, x_train, y_train):\n",
    "        \"\"\"Apply data augmentation to improve model robustness\"\"\"\n",
    "        datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "            rotation_range=10,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            shear_range=0.1\n",
    "        )\n",
    "        \n",
    "        return datagen.flow(x_train, y_train, batch_size=32)\n",
    "    \n",
    "    def train_model(self, epochs=15):\n",
    "        \"\"\"Train the improved model with data augmentation\"\"\"\n",
    "        print(\"Loading MNIST dataset...\")\n",
    "        (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        x_train = x_train.astype('float32') / 255.0\n",
    "        x_test = x_test.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape data\n",
    "        x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "        \n",
    "        # Create data augmentation generator\n",
    "        train_generator = self.augment_data(x_train, y_train)\n",
    "        \n",
    "        print(\"Training improved model...\")\n",
    "        history = self.model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=len(x_train) // 32,\n",
    "            epochs=epochs,\n",
    "            validation_data=(x_test, y_test),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_acc = self.model.evaluate(x_test, y_test, verbose=0)\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "        \n",
    "        self.is_trained = True\n",
    "        return history\n",
    "    \n",
    "    def save_model(self, filepath='improved_digit_model.h5'):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        if self.model and self.is_trained:\n",
    "            self.model.save(filepath)\n",
    "            print(f\"Model saved to {filepath}\")\n",
    "        else:\n",
    "            print(\"No trained model to save\")\n",
    "    \n",
    "    def load_model(self, filepath='improved_digit_model.h5'):\n",
    "        \"\"\"Load a pre-trained model\"\"\"\n",
    "        if os.path.exists(filepath):\n",
    "            self.model = keras.models.load_model(filepath)\n",
    "            self.is_trained = True\n",
    "            print(f\"Model loaded from {filepath}\")\n",
    "        else:\n",
    "            print(f\"Model file {filepath} not found\")\n",
    "    \n",
    "    def preprocess_digit_image(self, img):\n",
    "        \"\"\"Advanced preprocessing for better digit recognition\"\"\"\n",
    "        # Convert to grayscale if needed\n",
    "        if len(img.shape) == 3:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Apply Gaussian blur to reduce noise\n",
    "        img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "        \n",
    "        # Apply adaptive threshold for better binarization\n",
    "        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                   cv2.THRESH_BINARY_INV, 11, 2)\n",
    "        \n",
    "        # Find contours to locate the digit\n",
    "        contours, _ = cv2.findContours(img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if contours:\n",
    "            # Find the largest contour (assumed to be the digit)\n",
    "            largest_contour = max(contours, key=cv2.contourArea)\n",
    "            \n",
    "            # Get bounding box\n",
    "            x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "            \n",
    "            # Extract digit region with some padding\n",
    "            padding = 10\n",
    "            x = max(0, x - padding)\n",
    "            y = max(0, y - padding)\n",
    "            w = min(img.shape[1] - x, w + 2 * padding)\n",
    "            h = min(img.shape[0] - y, h + 2 * padding)\n",
    "            \n",
    "            digit_img = img[y:y+h, x:x+w]\n",
    "            \n",
    "            # Make the image square by adding padding\n",
    "            if w > h:\n",
    "                pad_top = (w - h) // 2\n",
    "                pad_bottom = w - h - pad_top\n",
    "                digit_img = np.pad(digit_img, ((pad_top, pad_bottom), (0, 0)), 'constant')\n",
    "            elif h > w:\n",
    "                pad_left = (h - w) // 2\n",
    "                pad_right = h - w - pad_left\n",
    "                digit_img = np.pad(digit_img, ((0, 0), (pad_left, pad_right)), 'constant')\n",
    "        else:\n",
    "            digit_img = img\n",
    "        \n",
    "        # Resize to 28x28\n",
    "        digit_img = cv2.resize(digit_img, (28, 28))\n",
    "        \n",
    "        # Apply morphological operations to clean up\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        digit_img = cv2.morphologyEx(digit_img, cv2.MORPH_CLOSE, kernel)\n",
    "        \n",
    "        # Normalize pixel values\n",
    "        digit_img = digit_img.astype('float32') / 255.0\n",
    "        \n",
    "        # Reshape for model input\n",
    "        digit_img = digit_img.reshape(1, 28, 28, 1)\n",
    "        \n",
    "        return digit_img\n",
    "    \n",
    "    def predict_digit_stable(self, img):\n",
    "        \"\"\"Predict digit with stability filtering\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return None, 0\n",
    "        \n",
    "        processed_img = self.preprocess_digit_image(img)\n",
    "        prediction = self.model.predict(processed_img, verbose=0)\n",
    "        digit = np.argmax(prediction)\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        # Add to prediction history for stability\n",
    "        self.prediction_history.append((digit, confidence))\n",
    "        \n",
    "        # Keep only last 5 predictions\n",
    "        if len(self.prediction_history) > 5:\n",
    "            self.prediction_history.pop(0)\n",
    "        \n",
    "        # Use majority vote for stable prediction\n",
    "        if len(self.prediction_history) >= 3:\n",
    "            recent_digits = [pred[0] for pred in self.prediction_history[-3:]]\n",
    "            recent_confidences = [pred[1] for pred in self.prediction_history[-3:]]\n",
    "            \n",
    "            # Check if we have consistent predictions\n",
    "            if len(set(recent_digits)) == 1 and min(recent_confidences) > 0.6:\n",
    "                return recent_digits[0], np.mean(recent_confidences)\n",
    "        \n",
    "        return digit, confidence\n",
    "    \n",
    "    def mouse_callback(self, event, x, y, flags, param):\n",
    "        \"\"\"Mouse callback for drawing on canvas\"\"\"\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            self.drawing = True\n",
    "        elif event == cv2.EVENT_MOUSEMOVE and self.drawing:\n",
    "            cv2.circle(self.canvas, (x, y), 8, 255, -1)\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            self.drawing = False\n",
    "    \n",
    "    def run_drawing_recognition(self):\n",
    "        \"\"\"Run digit recognition with drawing canvas\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model not trained! Please train the model first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting drawing-based digit recognition...\")\n",
    "        print(\"Instructions:\")\n",
    "        print(\"- Draw digits with your mouse in the window\")\n",
    "        print(\"- Press 'c' to clear the canvas\")\n",
    "        print(\"- Press 'p' to predict the current drawing\")\n",
    "        print(\"- Press 'q' to quit\")\n",
    "        \n",
    "        cv2.namedWindow('Draw Digit Here')\n",
    "        cv2.setMouseCallback('Draw Digit Here', self.mouse_callback)\n",
    "        \n",
    "        while True:\n",
    "            # Create display image\n",
    "            display_img = cv2.cvtColor(self.canvas, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            # Add instructions\n",
    "            cv2.putText(display_img, \"Draw a digit here\", (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            cv2.putText(display_img, \"Press 'p' to predict\", (10, 360), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            cv2.putText(display_img, \"Press 'c' to clear\", (10, 380), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "            \n",
    "            # Show last prediction\n",
    "            if self.last_prediction:\n",
    "                digit, confidence = self.last_prediction\n",
    "                cv2.putText(display_img, f\"Prediction: {digit} ({confidence:.2f})\", \n",
    "                           (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Draw Digit Here', display_img)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                self.canvas = np.zeros((400, 400), dtype=np.uint8)\n",
    "                self.last_prediction = None\n",
    "                self.prediction_history = []\n",
    "            elif key == ord('p'):\n",
    "                if np.sum(self.canvas) > 0:  # Check if something is drawn\n",
    "                    digit, confidence = self.predict_digit_stable(self.canvas)\n",
    "                    self.last_prediction = (digit, confidence)\n",
    "                    print(f\"Predicted: {digit} (confidence: {confidence:.3f})\")\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    def run_webcam_recognition(self):\n",
    "        \"\"\"Run improved webcam digit recognition\"\"\"\n",
    "        if not self.is_trained:\n",
    "            print(\"Model not trained! Please train the model first.\")\n",
    "            return\n",
    "        \n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Could not open webcam\")\n",
    "            return\n",
    "        \n",
    "        print(\"Starting improved webcam digit recognition...\")\n",
    "        print(\"Instructions:\")\n",
    "        print(\"- Hold up handwritten digits to the camera\")\n",
    "        print(\"- Keep digits steady for better recognition\")\n",
    "        print(\"- Press 'q' to quit\")\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Flip frame horizontally\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Define region of interest (center of frame)\n",
    "            h, w = frame.shape[:2]\n",
    "            roi_size = 200\n",
    "            x1 = (w - roi_size) // 2\n",
    "            y1 = (h - roi_size) // 2\n",
    "            x2 = x1 + roi_size\n",
    "            y2 = y1 + roi_size\n",
    "            \n",
    "            # Draw ROI rectangle\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Extract ROI\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            # Convert to grayscale\n",
    "            roi_gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Predict digit\n",
    "            digit, confidence = self.predict_digit_stable(roi_gray)\n",
    "            \n",
    "            # Display prediction only if confidence is high\n",
    "            if digit is not None and confidence > 0.7:\n",
    "                cv2.putText(frame, f\"Digit: {digit}\", (10, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 3)\n",
    "                cv2.putText(frame, f\"Confidence: {confidence:.2f}\", (10, 100), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Show processed image in corner\n",
    "            processed_roi = self.preprocess_digit_image(roi_gray)\n",
    "            processed_display = (processed_roi.reshape(28, 28) * 255).astype(np.uint8)\n",
    "            processed_display = cv2.resize(processed_display, (100, 100))\n",
    "            frame[10:110, w-110:w-10] = cv2.cvtColor(processed_display, cv2.COLOR_GRAY2BGR)\n",
    "            \n",
    "            # Add instructions\n",
    "            cv2.putText(frame, \"Hold digit in green box\", (10, h-40), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, \"Press 'q' to quit\", (10, h-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Improved Digit Recognition', frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function with mode selection\"\"\"\n",
    "    recognizer = ImprovedHandDigitRecognizer()\n",
    "    \n",
    "    print(\"Improved Hand Digit Recognition System\")\n",
    "    print(\"=====================================\")\n",
    "    \n",
    "    # Try to load existing model\n",
    "    recognizer.load_model()\n",
    "    \n",
    "    # If no model exists, create and train one\n",
    "    if not recognizer.is_trained:\n",
    "        print(\"No pre-trained model found. Training improved model...\")\n",
    "        recognizer.create_model()\n",
    "        recognizer.train_model(epochs=10)\n",
    "        recognizer.save_model()\n",
    "    \n",
    "    # Mode selection\n",
    "    print(\"\\nSelect mode:\")\n",
    "    print(\"1. Drawing mode (recommended for testing)\")\n",
    "    print(\"2. Webcam mode\")\n",
    "    \n",
    "    choice = input(\"Enter your choice (1 or 2): \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        recognizer.run_drawing_recognition()\n",
    "    elif choice == '2':\n",
    "        recognizer.run_webcam_recognition()\n",
    "    else:\n",
    "        print(\"Invalid choice. Starting drawing mode...\")\n",
    "        recognizer.run_drawing_recognition()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de0603-5b69-4d84-b1d8-061b5a04bde0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
